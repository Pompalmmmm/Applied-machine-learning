{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekaratnida/Applied-machine-learning/blob/master/Week03-MLR/Lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR9ZdTd2JONK"
      },
      "source": [
        "# Multivariate linear regresion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMd2JbV8JONN"
      },
      "source": [
        "## 1.1 Normal equation (Numpy version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "gTzq8DsWJONN",
        "outputId": "cf29f6e8-357e-4552-fb56-7bcbe48584bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 2)\n",
            "y =  [1 1 4]\n",
            "x_b =  [[1. 0. 1.]\n",
            " [1. 2. 6.]\n",
            " [1. 3. 8.]]\n",
            "theta =  [ 7. 15. -6.]\n",
            "y_p =  [1. 1. 4.]\n",
            "data =  [(0, 1), (2, 1), (3, 4)]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(precision=2)\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "#x, y = make_regression(n_samples=10, n_features=2, noise=2, random_state=123)\n",
        "#x_b = np.c_[np.ones((x.shape[0],1)),x]\n",
        "\n",
        "#x = np.array([[0,2,3],[1,6,8]]).T\n",
        "x = np.array([[0,1],[2,6],[3,8]])\n",
        "y = np.array([1,1,4])\n",
        "print(x.shape)\n",
        "print(\"y = \", y)\n",
        "\n",
        "x_b = np.c_[np.ones((x.shape[0],1)),x]\n",
        "print(\"x_b = \", x_b)\n",
        "\n",
        "#call inverse function from linear algebra module\n",
        "theta = np.linalg.inv(x_b.T.dot(x_b)).dot(x_b.T).dot(y)\n",
        "print(\"theta = \", theta)\n",
        "\n",
        "#predict trainned x\n",
        "y_p = x_b.dot(theta)\n",
        "print(\"y_p = \",y_p)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Create the title\n",
        "exp = \"\"\n",
        "plus = \"+\"\n",
        "for t in range(len(theta)):\n",
        "  if t == len(theta)-1:\n",
        "    plus = \"\"\n",
        "  if t == 0:\n",
        "    xterm = \"\"\n",
        "  else:\n",
        "    xterm = r\"$x_\"+str(t)+\"$\"\n",
        "  exp += str(np.round(theta[t],2)) + xterm + plus\n",
        "#end create the title\n",
        "\n",
        "plt.title(\"y = \"+exp)\n",
        "data = sorted(zip(x[:,0],y))\n",
        "print(\"data = \", data)\n",
        "x_val = [x[0] for x in data]\n",
        "y_val = [x[1] for x in data]\n",
        "\n",
        "plt.plot(x_val, y_val, \"b.\")\n",
        "#plt.plot(x_val, y_p,\"r.\")\n",
        "plt.xlabel(r'$x_1$')\n",
        "plt.ylabel(r'$y$')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = sorted(zip(x[:,0],y))\n",
        "print(type(test))\n",
        "for i in test:\n",
        "  print(i)\n",
        "\n",
        "test = zip(x[:,0],y)\n",
        "print(type(test))\n",
        "for i in test:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "mzOqCBTd7JTs",
        "outputId": "aac93e10-e011-414c-8edb-49524b028236",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "(0, 1)\n",
            "(2, 1)\n",
            "(3, 4)\n",
            "<class 'zip'>\n",
            "(0, 1)\n",
            "(2, 1)\n",
            "(3, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHMj49QGJONQ"
      },
      "source": [
        "Exercise1 <br>\n",
        "1.1 Change a number of features to 4 (x1,x2,x3,x4) and plot 4 graphs </br>\n",
        "1.2 Change a number of features to 2 (x1,x2) and use plotly to plot 3D visualization (x1,x2,y)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEsehC1aJONR"
      },
      "source": [
        "# **Sklearn**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "K0nd1HtqJONR",
        "outputId": "74f385ee-e8bc-46e4-e326-2ff4e32e3f5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.999999999999989  ,  [15. -6.]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3ec4e76f7f28>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#x_n = np.array([[-3,3],[5,2]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0my_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlin_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y predict = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x_n' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#x, y = make_regression(n_samples=1000, n_features=2, n_informative=1, random_state=0, noise=100)\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(x, y)\n",
        "print(lin_reg.intercept_, \" , \", lin_reg.coef_)\n",
        "\n",
        "\n",
        "#prediction\n",
        "#x_n = np.array([[-3,3],[5,2]])\n",
        "y_p = lin_reg.predict(x_n)\n",
        "print(\"y predict = \",y_p)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(x_n[:,0], y_p,\"r-\")\n",
        "plt.plot(x[:,0],y, \"b.\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(x_n[:,1], y_p,\"r-\")\n",
        "plt.plot(x[:,1],y, \"b.\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(x_n[:,2], y_p,\"r-\")\n",
        "plt.plot(x[:,2],y, \"b.\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elLXFrvqJONS"
      },
      "source": [
        "# **Batch Gradient descent (Multiple linear regression)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fy_puf-oJONS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "def cost_function(theta, x, y, N):\n",
        "\n",
        "    y_hat = theta.T.dot(x.T)\n",
        "    #print(\"y_predict \",y_hat)\n",
        "    hmse = (1/(2*N))*np.sum((y_hat-y)**2)\n",
        "    #print(\"error \", hmse)\n",
        "    return hmse\n",
        "\n",
        "def gradient_descent(alpha, x, y, ep=0.0000001, max_iter=10000):\n",
        "\n",
        "    converged = False\n",
        "    iter = 0\n",
        "    N = x.shape[0] # number of samples\n",
        "\n",
        "    # initial theta\n",
        "    t = np.random.random((x.shape[1],1))\n",
        "    #t = np.ones((x.shape[1],1))\n",
        "    #print(\"t \", t)\n",
        "\n",
        "    # total error, J(theta)\n",
        "    J = cost_function(t,x,y,N)\n",
        "    #print(\"Iteration 0 --> J=\",J,\" t0=\",t0,\" t1=\",t1)\n",
        "\n",
        "    # Iterate Loop\n",
        "    while not converged:\n",
        "\n",
        "        #print(\"theta.shape \",t.shape)\n",
        "        #print(\"x.shape \",x.shape)\n",
        "        y_hat = t.T.dot(x.T)\n",
        "        error = y_hat-y\n",
        "        grad = x.T.dot(error.T)\n",
        "\n",
        "        #grad = x.T.dot(((t.T.dot(x.T)-y).T))\n",
        "\n",
        "        t = t - alpha * (1/N) * (grad)\n",
        "\n",
        "        # error\n",
        "        e = cost_function(t,x,y,N)\n",
        "\n",
        "        if abs(J-e) <= ep:\n",
        "            print(\"Converged, iterations: \", iter, \"/\", max_iter)\n",
        "            converged = True\n",
        "\n",
        "        J = e   # update error s\n",
        "        iter += 1  # update iter\n",
        "\n",
        "        if iter == max_iter:\n",
        "            print('Max iterations exceeded!')\n",
        "            converged = True\n",
        "\n",
        "    return t\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    alpha = 0.01 # learning rate\n",
        "\n",
        "    theta_bgd = gradient_descent(alpha, x_b, y, max_iter=100000)\n",
        "    print (\"theta \", theta_bgd)\n",
        "\n",
        "    y_p = x_n_b.dot(theta_bgd)\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.plot(x_n[:,0], y_p,\"r-\")\n",
        "    plt.plot(x[:,0],y, \"b.\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    #Evaluation\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    from sklearn.metrics import r2_score\n",
        "    y3_p = x_b.dot(theta_bgd)\n",
        "\n",
        "    print(\"BGD\")\n",
        "    print(\"Mean of Squared Errors = \", mean_squared_error(y, y3_p))\n",
        "    #print(\"Mean of Absolute Errors = \", mean_absolute_error(x_b[:,1], y2_p))\n",
        "    print(\"R2 score = \", r2_score(y, y3_p))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3LSmwS-JONU"
      },
      "source": [
        "# Stochastic GD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXxp5V1mJONU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import time\n",
        "\n",
        "\n",
        "def cost_function(theta, x, y, m):\n",
        "    y_predict = theta.T.dot(x.T)\n",
        "    error = np.sum((y_predict-y)**2)\n",
        "    return error\n",
        "\n",
        "def gradient_descent(alpha, x, y, ep=0.0000001, max_iter=100000):\n",
        "\n",
        "    converged = False\n",
        "    iter = 0\n",
        "    m = x.shape[0] # number of samples\n",
        "\n",
        "    # initial theta\n",
        "    t = np.random.random((x.shape[1],1))\n",
        "\n",
        "    # total error, J(theta)\n",
        "    J = cost_function(t,x,y,m)\n",
        "\n",
        "    # Iterate Loop\n",
        "    while not converged:\n",
        "\n",
        "\n",
        "        rand_ind = np.random.randint(0,m)\n",
        "        #print(rand_ind)\n",
        "        X_i = x[rand_ind,:]\n",
        "        y_i = y[rand_ind].reshape(1,1)\n",
        "\n",
        "        y_predict = t.T.dot(X_i)\n",
        "        error = y_predict-y_i\n",
        "        X_i = X_i.reshape(1,x.shape[1])\n",
        "        grad = X_i.T.dot(error.T)\n",
        "\n",
        "        alpha = 0.001 #Learning rate\n",
        "        decay = alpha/max_iter\n",
        "        alpha *= 1/(1+decay*iter)\n",
        "        #print(\"learning schedule = \", alpha)\n",
        "\n",
        "        t = t - alpha * (1/1) * (grad)\n",
        "\n",
        "        # error\n",
        "        e = cost_function(t,X_i,y_i,m)\n",
        "\n",
        "        if abs(J-e) <= ep:\n",
        "            print(\"Converged, iterations: \", iter, \"/\", max_iter)\n",
        "            converged = True\n",
        "\n",
        "        J = e   # update error s\n",
        "        iter += 1  # update iter\n",
        "\n",
        "        if iter == max_iter:\n",
        "            print('Max iterations exceeded!')\n",
        "            converged = True\n",
        "\n",
        "    return t\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    #x, y = make_regression(n_samples=100, n_features=2, n_informative=1, random_state=0, noise=35)\n",
        "    #x_b = np.c_[np.ones((x.shape[0],1)),x]\n",
        "\n",
        "    alpha = 0.01 # learning rate\n",
        "\n",
        "    theta_sgd = gradient_descent(alpha, x_b, y, max_iter=100000)\n",
        "    print (\"theta \", theta_sgd)\n",
        "\n",
        "    y_p = x_n_b.dot(theta_sgd)\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.plot(x_n[:,0], y_p,\"r-\")\n",
        "    plt.plot(x[:,0],y, \"b.\")\n",
        "    plt.show()\n",
        "\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    from sklearn.metrics import mean_absolute_error\n",
        "    from sklearn.metrics import r2_score\n",
        "    y2_p = x_b.dot(theta_sgd)\n",
        "    #print(\"y predict = \",y2_p)\n",
        "\n",
        "    print(\"SGD\")\n",
        "    print(\"Mean of Squared Errors = \", mean_squared_error(y, y2_p))\n",
        "    #print(\"Mean of Absolute Errors = \", mean_absolute_error(x_b[:,1], y2_p))\n",
        "    print(\"R2 score = \", r2_score(y, y2_p))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDicYUpzJONV"
      },
      "source": [
        "# Mini-batch GD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNkqLNRuJONV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import time\n",
        "\n",
        "def cost_function(theta, x, y, m):\n",
        "    y_predict = theta.T.dot(x.T)\n",
        "    error = np.sum((y_predict-y)**2)\n",
        "    return error\n",
        "\n",
        "def gradient_descent(alpha, x, y, ep=0.00000001, max_iter=10000):\n",
        "\n",
        "    converged = False\n",
        "    iter = 0\n",
        "    m = x.shape[0] # number of samples\n",
        "\n",
        "    # initial theta\n",
        "    t = np.random.random((x.shape[1],1))\n",
        "\n",
        "    # total error, J(theta)\n",
        "    J = cost_function(t,x,y,m)\n",
        "\n",
        "    batch_size = 10\n",
        "\n",
        "    # Iterate Loop\n",
        "    while not converged:\n",
        "\n",
        "        #shuffle\n",
        "        rand_ind = np.random.permutation(m)\n",
        "        #print(rand_ind)\n",
        "        x = x[rand_ind]\n",
        "        y = y[rand_ind]\n",
        "\n",
        "        for i in range(0,m,batch_size):\n",
        "            #print(\"i\",i)\n",
        "\n",
        "            X_i = x[i:i+batch_size]\n",
        "            #print(\"X_i\", X_i)\n",
        "            y_i = y[i:i+batch_size]\n",
        "            #print(\"y_i\", y_i)\n",
        "\n",
        "            y_predict = t.T.dot(X_i.T)\n",
        "            error = y_predict-y_i\n",
        "            grad = X_i.T.dot(error.T)\n",
        "\n",
        "            t = t - alpha * (1/batch_size) * (grad)\n",
        "\n",
        "            # error\n",
        "            e = cost_function(t,X_i,y_i,m)\n",
        "\n",
        "            if abs(J-e) <= ep:\n",
        "                print(\"Converged, iterations: \", iter, \"/\", max_iter)\n",
        "                converged = True\n",
        "\n",
        "            J = e   # update error s\n",
        "            iter += 1  # update iter\n",
        "\n",
        "            if iter == max_iter:\n",
        "                print('Max iterations exceeded!')\n",
        "                converged = True\n",
        "\n",
        "    return t\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    alpha = 0.01 # learning rate\n",
        "\n",
        "    theta = gradient_descent(alpha, x_b, y, max_iter=100000)\n",
        "    print (\"theta \", theta)\n",
        "\n",
        "    y_p = x_n_b.dot(theta)\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.plot(x_n[:,0], y_p,\"r-\")\n",
        "    plt.plot(x[:,0],y, \"b.\")\n",
        "    plt.show()\n",
        "\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    #from sklearn.metrics import mean_absolute_error\n",
        "    from sklearn.metrics import r2_score\n",
        "    y2_p = x_b.dot(theta_sgd)\n",
        "    #print(\"y predict = \",y2_p)\n",
        "\n",
        "    print(\"miniGD\")\n",
        "    print(\"Mean of Squared Errors = \", mean_squared_error(y, y2_p))\n",
        "    #print(\"Mean of Absolute Errors = \", mean_absolute_error(x_b[:,1], y2_p))\n",
        "    print(\"R2 score = \", r2_score(y, y2_p))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BvXPq1FJONW"
      },
      "source": [
        "# Polynomial regression\n",
        "Reference: https://towardsdatascience.com/machine-learning-polynomial-regression-with-python-5328e4e8a386"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFoOB7-1JONW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Importing the dataset\n",
        "df = pd.read_csv('https://s3.us-west-2.amazonaws.com/public.gamelab.fun/dataset/position_salaries.csv')\n",
        "X = df.iloc[:, 1:2].values\n",
        "y = df.iloc[:, 2].values\n",
        "print(df.head())\n",
        "print(X)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ksf7VFwaJONX"
      },
      "outputs": [],
      "source": [
        "# Fitting Linear Regression to the dataset\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X, y)\n",
        "print(y)\n",
        "# Visualizing the Linear Regression results\n",
        "\n",
        "plt.scatter(X, y, color='red')\n",
        "plt.plot(X, lin_reg.predict(X), color='blue')\n",
        "plt.title('Truth or Bluff (Linear Regression)')\n",
        "plt.xlabel('Position level')\n",
        "plt.ylabel('Salary')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhivx0cQJONX"
      },
      "outputs": [],
      "source": [
        "# Fitting Polynomial Regression to the dataset\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly_reg = PolynomialFeatures(degree=2)\n",
        "X_poly = poly_reg.fit_transform(X)\n",
        "print(X_poly)\n",
        "pol_reg = LinearRegression()\n",
        "pol_reg.fit(X_poly, y)\n",
        "\n",
        "# Visualizing the Polymonial Regression results\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(X, y, color='red')\n",
        "plt.plot(X, lin_reg.predict(X), color='green')\n",
        "plt.plot(X, pol_reg.predict(X_poly), color='blue')\n",
        "plt.title('Truth or Bluff (Linear Regression)')\n",
        "plt.xlabel('Position level')\n",
        "plt.ylabel('Salary')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0Vog6EbJONY"
      },
      "outputs": [],
      "source": [
        "# Predicting a new result with Linear Regression\n",
        "print(lin_reg.predict([[5.5]]))\n",
        "#output should be 249500\n",
        "\n",
        "# Predicting a new result with Polymonial Regression\n",
        "print(pol_reg.predict(poly_reg.fit_transform([[5.5]])))\n",
        "#output should be 132148.43750003"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#timeit, m=1M\n",
        "#m=10, print line by line"
      ],
      "metadata": {
        "id": "z7AL91RTrMjN"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Lab3.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}